{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes\n",
    "\n",
    "Bayes' Theorem is a principle in probability theory and statistics that describes the probability of an event, based on prior knowledge of conditions related to the event. Named after Reverend Thomas Bayes, it provides a way to update probability estimates based on new evidence.\n",
    "\n",
    "The formula is usually given as:\n",
    "\n",
    "\\[\n",
    "P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\n",
    "\\]\n",
    "\n",
    "### Components:\n",
    "\n",
    "- \\( P(A|B) \\) is the posterior probability of \\( A \\) given \\( B \\).\n",
    "- \\( P(B|A) \\) is the likelihood, the probability of observing \\( B \\) given \\( A \\).\n",
    "- \\( P(A) \\) is the prior probability, or marginal probability, of \\( A \\).\n",
    "- \\( P(B) \\) is the marginal probability of \\( B \\), and can be found by summing \\( P(B|A) \\times P(A) \\) over all values of \\( A \\).\n",
    "\n",
    "### Common Use Cases:\n",
    "\n",
    "1. **Medical Diagnosis**: Estimating the probability of a disease given a test result.\n",
    "2. **Spam Filtering**: Determining if an email is spam based on its content.\n",
    "3. **Recommendation Systems**: Predicting user behavior based on past actions.\n",
    "  \n",
    "### Simple Example:\n",
    "\n",
    "Let's say there are two urns, one with 3 red balls and 7 green balls (Urn A), and another with 5 red and 5 green (Urn B). You randomly choose an urn and then randomly pick a ball, which turns out to be red. What's the probability that you chose Urn A?\n",
    "\n",
    "Here:\n",
    "\n",
    "- \\( A \\) = You chose Urn A\n",
    "- \\( B \\) = You drew a red ball\n",
    "\n",
    "So using Bayes' Theorem:\n",
    "\n",
    "\\[\n",
    "P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\n",
    "\\]\n",
    "\\[\n",
    "P(A|B) = \\frac{(3/10) \\times (1/2)}{(4/10 \\times 1/2) + (5/10 \\times 1/2)}\n",
    "\\]\n",
    "\\[\n",
    "P(A|B) = \\frac{0.15}{0.225} \\approx 0.67\n",
    "\\]\n",
    "\n",
    "So the probability that you chose Urn A, given that you drew a red ball, is approximately 0.67."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of having chosen Urn A given that we drew a red ball is: 0.37\n"
     ]
    }
   ],
   "source": [
    "def bayes_theorem(p_a, p_b_given_a, p_b_given_not_a):\n",
    "    # Calculate P(not A)\n",
    "    p_not_a = 1 - p_a\n",
    "    \n",
    "    # Calculate P(B)\n",
    "    p_b = p_b_given_a * p_a + p_b_given_not_a * p_not_a\n",
    "    \n",
    "    # Calculate P(A|B)\n",
    "    p_a_given_b = (p_b_given_a * p_a) / p_b\n",
    "    \n",
    "    return p_a_given_b\n",
    "\n",
    "# Probability of choosing Urn A (P(A))\n",
    "p_a = 0.5\n",
    "\n",
    "# Probability of drawing a red ball from Urn A (P(B|A))\n",
    "p_b_given_a = 3 / 10\n",
    "\n",
    "# Probability of drawing a red ball from Urn B (P(B|not A))\n",
    "p_b_given_not_a = 5 / 10\n",
    "\n",
    "# Calculate P(A|B)\n",
    "p_a_given_b = bayes_theorem(p_a, p_b_given_a, p_b_given_not_a)\n",
    "\n",
    "print(f\"The probability of having chosen Urn A given that we drew a red ball is: {p_a_given_b:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Certainly! Naive Bayes is a supervised learning algorithm based on applying Bayes' theorem, with the \"naive\" assumption that features are independent of each other. In other words, it assumes that the presence of a particular feature in a class is not related to the presence of any other feature.\n",
    "\n",
    "It's particularly good for text classification tasks like spam filtering, sentiment analysis, and category tagging. But it's also used in a variety of applications including product recommendation, fraud detection, and medical diagnosis.\n",
    "\n",
    "The algorithm is simple and computationally efficient, making it suitable for high-dimensional or large-volume datasets. There are different variants like Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes, which are suited for different types of data.\n",
    "\n",
    "### Basic Example in Python using scikit-learn:\n",
    "\n",
    "Let's say we have a simple dataset for classifying fruits based on their \"weight\" and \"texture\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "# Features (weight, texture: 0=smooth, 1=bumpy)\n",
    "X = np.array([[140, 0],\n",
    "              [130, 0],\n",
    "              [150, 1],\n",
    "              [170, 1]])\n",
    "\n",
    "# Labels (0=apple, 1=orange)\n",
    "y = np.array([0, 0, 1, 1])\n",
    "\n",
    "# Create the classifier\n",
    "clf = GaussianNB()\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Make a prediction\n",
    "prediction = clf.predict([[145, 1]])\n",
    "\n",
    "print(\"Prediction:\", prediction)  # Should print [1] for orange"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
